{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misclassification on \"none\" outputs:\n",
    "We modified the wikisql dataset in \"no_answer_questions.ipynb\" and ran the tapex.base and tapex.large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate misclassification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives (TP): 1256\n",
      "False Positives (FP): 237\n",
      "True Negatives (TN): 17187\n",
      "False Negatives (FN): 372\n",
      "Total 'none' ground truth: 1628\n",
      "Recall: 77.15%\n",
      "Precision: 84.13%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_none_predictions(eval_file_path):\n",
    "    # Read the file into a DataFrame\n",
    "    df = pd.read_csv(eval_file_path, sep='\\t')\n",
    "\n",
    "    # Calculate True Positives (TP), False Positives (FP), True Negatives (TN), False Negatives (FN)\n",
    "    tp = df[(df['Predict'] == 'none') & (df['Golden'] == 'none')].shape[0]  # TP: Predict 'none' and Golden 'none'\n",
    "    fp = df[(df['Predict'] == 'none') & (df['Golden'] != 'none')].shape[0]  # FP: Predict 'none', Golden not 'none'\n",
    "    tn = df[(df['Predict'] != 'none') & (df['Golden'] != 'none')].shape[0]  # TN: Predict not 'none', Golden not 'none'\n",
    "    fn = df[(df['Predict'] != 'none') & (df['Golden'] == 'none')].shape[0]  # FN: Predict not 'none', Golden 'none'\n",
    "\n",
    "    # Calculate recall (TP / (TP + FN)) and precision (TP / (TP + FP))\n",
    "    recall = (tp / (tp + fn)) * 100 if (tp + fn) > 0 else 0\n",
    "    precision = (tp / (tp + fp)) * 100 if (tp + fp) > 0 else 0\n",
    "\n",
    "    # Calculate the total number of \"none\" golden rows (i.e., total \"none\" ground truth instances)\n",
    "    total_none_golden = df[df['Golden'] == 'none'].shape[0]\n",
    "\n",
    "    return tp, fp, tn, fn, total_none_golden, recall, precision\n",
    "\n",
    "# Example usage:\n",
    "eval_file_path = \"/Users/sebastiaan/Desktop/IR2_tapex_reproducibility_study/src/Table-Pretraining-main/results/wikisql/tapex.base/test/generate-test.txt.eval\"\n",
    "tp, fp, tn, fn, total_none_golden, recall, precision = calculate_none_predictions(eval_file_path)\n",
    "\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"Total 'none' ground truth: {total_none_golden}\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tapex.base TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives (TP): 1256\n",
      "False Positives (FP): 237\n",
      "True Negatives (TN): 17187\n",
      "False Negatives (FN): 372\n",
      "Total 'none' ground truth: 1628\n",
      "Recall: 77.15%\n",
      "Precision: 84.13%\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "eval_file_path = \"/Users/sebastiaan/Desktop/IR2_tapex_reproducibility_study/src/Table-Pretraining-main/results/wikisql/tapex.base/test/generate-test.txt.eval\"\n",
    "tp, fp, tn, fn, total_none_golden, recall, precision = calculate_none_predictions(eval_file_path)\n",
    "\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"Total 'none' ground truth: {total_none_golden}\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tapex.large TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives (TP): 1375\n",
      "False Positives (FP): 253\n",
      "True Negatives (TN): 17171\n",
      "False Negatives (FN): 253\n",
      "Total 'none' ground truth: 1628\n",
      "Recall: 84.46%\n",
      "Precision: 84.46%\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "large_file_path = \"/Users/sebastiaan/Desktop/IR2_tapex_reproducibility_study/src/Table-Pretraining-main/results/wikisql/tapex.large/test/generate-test.txt.eval\"\n",
    "tp, fp, tn, fn, total_none_golden, recall, precision = calculate_none_predictions(large_file_path)\n",
    "\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"Total 'none' ground truth: {total_none_golden}\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_none_scores(eval_file_path):\n",
    "    # Read the file into a DataFrame\n",
    "    df = pd.read_csv(eval_file_path, sep='\\t')\n",
    "\n",
    "    # True positive: Predicted \"none\" and Golden \"none\"\n",
    "    tp = df[(df['Predict'] == 'none') & (df['Golden'] == 'none')].shape[0]\n",
    "\n",
    "    # False positive: Predicted \"none\" and Golden not \"none\"\n",
    "    fp = df[(df['Predict'] == 'none') & (df['Golden'] != 'none')].shape[0]\n",
    "\n",
    "    # False negative: Predicted not \"none\" and Golden \"none\"\n",
    "    fn = df[(df['Predict'] != 'none') & (df['Golden'] == 'none')].shape[0]\n",
    "\n",
    "    # True negative: Predicted not \"none\" and Golden not \"none\"\n",
    "    tn = df[(df['Predict'] != 'none') & (df['Golden'] != 'none')].shape[0]\n",
    "\n",
    "    # Calculate the precision, recall, and F1 score\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn) if tp + fp + fn + tn > 0 else 0\n",
    "\n",
    "    return tp, fp, fn, tn, precision, recall, f1, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAPEX.BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAPEX.BASE VAL:\n",
      "TP: 635\n",
      "FP: 102\n",
      "FN: 173\n",
      "TN: 9194\n",
      "Precision: 0.86\n",
      "Recall: 0.79\n",
      "F1: 0.82\n",
      "Accuracy: 0.97\n",
      "\n",
      "\n",
      "TAPEX.BASE TEST:\n",
      "TP: 1256\n",
      "FP: 237\n",
      "FN: 372\n",
      "TN: 17187\n",
      "Precision: 0.84\n",
      "Recall: 0.77\n",
      "F1: 0.80\n",
      "Accuracy: 0.97\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the paths to the generate-valid.txt.eval and generate-test.txt.eval files\n",
    "base_valid_eval_file_path = \"/Users/sebastiaan/Desktop/IR2_tapex_reproducibility_study/src/Table-Pretraining-main/results/wikisql/tapex.base/val/generate-valid.txt.eval\"\n",
    "base_test_eval_file_path = \"/Users/sebastiaan/Desktop/IR2_tapex_reproducibility_study/src/Table-Pretraining-main/results/wikisql/tapex.base/test/generate-test.txt.eval\"\n",
    "\n",
    "# Calculate and print results for the valid set on tapex.base\n",
    "valid_tp, valid_fp, valid_fn, valid_tn, valid_precision, valid_recall, valid_f1, valid_accuracy = calculate_none_scores(base_valid_eval_file_path)\n",
    "print(\"TAPEX.BASE VAL:\")\n",
    "print(f\"TP: {valid_tp}\")\n",
    "print(f\"FP: {valid_fp}\")\n",
    "print(f\"FN: {valid_fn}\")\n",
    "print(f\"TN: {valid_tn}\")\n",
    "print(f\"Precision: {valid_precision:.2f}\")\n",
    "print(f\"Recall: {valid_recall:.2f}\")\n",
    "print(f\"F1: {valid_f1:.2f}\")\n",
    "print(f\"Accuracy: {valid_accuracy:.2f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Calculate and print results for the test set on tapex.base\n",
    "test_tp, test_fp, test_fn, test_tn, test_precision, test_recall, test_f1, test_accuracy = calculate_none_scores(base_test_eval_file_path)\n",
    "print(\"TAPEX.BASE TEST:\")\n",
    "print(f\"TP: {test_tp}\")\n",
    "print(f\"FP: {test_fp}\")\n",
    "print(f\"FN: {test_fn}\")\n",
    "print(f\"TN: {test_tn}\")\n",
    "print(f\"Precision: {test_precision:.2f}\")\n",
    "print(f\"Recall: {test_recall:.2f}\")\n",
    "print(f\"F1: {test_f1:.2f}\")\n",
    "print(f\"Accuracy: {test_accuracy:.2f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAPEX.LARGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAPEX.LARGE VAL:\n",
      "TP: 666\n",
      "FP: 127\n",
      "FN: 142\n",
      "TN: 9169\n",
      "Precision: 0.84\n",
      "Recall: 0.82\n",
      "F1: 0.83\n",
      "Accuracy: 0.97\n",
      "\n",
      "\n",
      "TAPEX.LARGE TEST:\n",
      "TP: 1375\n",
      "FP: 253\n",
      "FN: 253\n",
      "TN: 17171\n",
      "Precision: 0.84\n",
      "Recall: 0.84\n",
      "F1: 0.84\n",
      "Accuracy: 0.97\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the paths to the generate-valid.txt.eval and generate-test.txt.eval files\n",
    "large_valid_eval_file_path = \"/Users/sebastiaan/Desktop/IR2_tapex_reproducibility_study/src/Table-Pretraining-main/results/wikisql/tapex.large/val/generate-valid.txt.eval\"\n",
    "large_test_eval_file_path = \"/Users/sebastiaan/Desktop/IR2_tapex_reproducibility_study/src/Table-Pretraining-main/results/wikisql/tapex.large/test/generate-test.txt.eval\"\n",
    "\n",
    "# Calculate and print results for the valid set on tapex.large\n",
    "valid_tp, valid_fp, valid_fn, valid_tn, valid_precision, valid_recall, valid_f1, valid_accuracy = calculate_none_scores(large_valid_eval_file_path)\n",
    "print(\"TAPEX.LARGE VAL:\")\n",
    "print(f\"TP: {valid_tp}\")\n",
    "print(f\"FP: {valid_fp}\")\n",
    "print(f\"FN: {valid_fn}\")\n",
    "print(f\"TN: {valid_tn}\")\n",
    "print(f\"Precision: {valid_precision:.2f}\")\n",
    "print(f\"Recall: {valid_recall:.2f}\")\n",
    "print(f\"F1: {valid_f1:.2f}\")\n",
    "print(f\"Accuracy: {valid_accuracy:.2f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Calculate and print results for the test set on tapex.large\n",
    "test_tp, test_fp, test_fn, test_tn, test_precision, test_recall, test_f1, test_accuracy = calculate_none_scores(large_test_eval_file_path)\n",
    "print(\"TAPEX.LARGE TEST:\")\n",
    "print(f\"TP: {test_tp}\")\n",
    "print(f\"FP: {test_fp}\")\n",
    "print(f\"FN: {test_fn}\")\n",
    "print(f\"TN: {test_tn}\")\n",
    "print(f\"Precision: {test_precision:.2f}\")\n",
    "print(f\"Recall: {test_recall:.2f}\")\n",
    "print(f\"F1: {test_f1:.2f}\")\n",
    "print(f\"Accuracy: {test_accuracy:.2f}\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Predicted: united states True: united states ', ' Predicted: arkansas True: arkansas ', ' Predicted: michigan True: michigan ', ' Predicted: 1.0 True: 1.0 ', ' Predicted: assen True: netherlands ', ' Predicted: 1.0 True: 1.0 ', ' Predicted: 17 june True: 17 june ', ' Predicted: 1.0 True: 1.0 ', ' Predicted: canada True: canada ', ' Predicted: 1.0 True: 1.0 ']\n",
      "Processed data points (first 10): [['united states', 'united states'], ['arkansas', 'arkansas'], ['michigan', 'michigan'], ['1.0', '1.0'], ['assen', 'netherlands'], ['1.0', '1.0'], ['17 june', '17 june'], ['1.0', '1.0'], ['canada', 'canada'], ['1.0', '1.0']]\n",
      "Total number of 'none' predictions (TP + FN): 1627\n",
      "Correct 'none' predictions (True Positives): 1313\n",
      "Percentage of correct 'none' predictions (Recall): 80.70%\n",
      "Recall (TP / (TP + FN)): 0.807\n",
      "Precision (TP / (TP + FP)): 0.866\n",
      "True Positives (predicted 'none' when actual 'none'): 1313\n",
      "False Positives (predicted 'none' when not 'none'): 204\n",
      "False Negatives (predicted something else when true 'none'): 314\n",
      "True Negatives (predicted something else when not 'none'): 17222\n"
     ]
    }
   ],
   "source": [
    "def analyze_predictions(file_path):\n",
    "    # Initialize counters for the different categories\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    true_negatives = 0\n",
    "    total_none = 0  # This counts how many times \"none\" is the true label\n",
    "    correct_none_predictions = 0  # To count correct 'none' predictions (True Positives)\n",
    "\n",
    "    # Create a list to store the predictions and true labels as lists\n",
    "    data_points = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read the entire file content\n",
    "        content = file.read()\n",
    "        \n",
    "        # Clean up the content by removing unwanted newline characters and splitting by the separator\n",
    "        content = content.replace('\\n', ' ').strip()  # Remove all '\\n' and strip extra spaces\n",
    "        data_point_strings = content.split('--------------------------------------------------')  # Split by the separator\n",
    "\n",
    "        print(data_point_strings[:10])  # To check the first few cleaned data points\n",
    "\n",
    "        # Iterate over each data point string\n",
    "        for data_point_string in data_point_strings:\n",
    "            # Strip leading/trailing whitespaces\n",
    "            data_point_string = data_point_string.strip()\n",
    "\n",
    "            if not data_point_string:\n",
    "                continue  # Skip empty data points\n",
    "\n",
    "            # Split the data point into predicted and true labels\n",
    "            if 'Predicted: ' in data_point_string and 'True: ' in data_point_string:\n",
    "                # Extract the predicted label and true label by splitting on the markers\n",
    "                predicted_label = data_point_string.split('Predicted: ')[1].split('True: ')[0].strip()\n",
    "                true_label = data_point_string.split('True: ')[1].strip()\n",
    "\n",
    "                # Append the data point as a list of predicted and true labels\n",
    "                data_points.append([predicted_label, true_label])\n",
    "\n",
    "    print(f\"Processed data points (first 10): {data_points[:10]}\")\n",
    "\n",
    "    # Iterate over the data points and calculate TP, FP, FN, TN\n",
    "    for predicted_label, true_label in data_points:\n",
    "        # Count true \"none\"\n",
    "        if true_label == \"none\":\n",
    "            total_none += 1\n",
    "            # If predicted 'none', it's a true positive\n",
    "            if predicted_label == \"none\":\n",
    "                correct_none_predictions += 1\n",
    "\n",
    "        # Calculate True Positives, False Positives, False Negatives, True Negatives\n",
    "        if true_label == \"none\" and predicted_label == \"none\":\n",
    "            true_positives += 1\n",
    "        elif true_label == \"none\" and predicted_label != \"none\":\n",
    "            false_negatives += 1\n",
    "        elif true_label != \"none\" and predicted_label == \"none\":\n",
    "            false_positives += 1\n",
    "        elif true_label != \"none\" and predicted_label != \"none\":\n",
    "            true_negatives += 1\n",
    "\n",
    "    # Calculate Recall: TP / (TP + FN)\n",
    "    recall = (true_positives / (true_positives + false_negatives)) if (true_positives + false_negatives) > 0 else 0\n",
    "\n",
    "    # Calculate Precision: TP / (TP + FP)\n",
    "    precision = (true_positives / (true_positives + false_positives)) if (true_positives + false_positives) > 0 else 0\n",
    "\n",
    "    # Calculate percentage of correct 'none' predictions\n",
    "    percentage_correct_none = (correct_none_predictions / total_none * 100) if total_none > 0 else 0\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Total number of 'none' predictions (TP + FN): {total_none}\")\n",
    "    print(f\"Correct 'none' predictions (True Positives): {correct_none_predictions}\")\n",
    "    print(f\"Percentage of correct 'none' predictions (Recall): {percentage_correct_none:.2f}%\")\n",
    "    print(f\"Recall (TP / (TP + FN)): {recall:.3f}\")\n",
    "    print(f\"Precision (TP / (TP + FP)): {precision:.3f}\")\n",
    "    print(f\"True Positives (predicted 'none' when actual 'none'): {true_positives}\")\n",
    "    print(f\"False Positives (predicted 'none' when not 'none'): {false_positives}\")\n",
    "    print(f\"False Negatives (predicted something else when true 'none'): {false_negatives}\")\n",
    "    print(f\"True Negatives (predicted something else when not 'none'): {true_negatives}\")\n",
    "\n",
    "\n",
    "# Replace 'path_to_file.txt' with the actual path to your .txt file\n",
    "file_path = '/Users/sebastiaan/Desktop/IR2_tapex_reproducibility_study/src/Table-Pretraining-main/results/wikisql/llama/test_predictions_wikisql_adverserial_epoch_3_markdown.txt'\n",
    "analyze_predictions(file_path)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
